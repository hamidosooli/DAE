program: onpolicy/scripts/wandb_sweep_wrapper.py
method: random
metric:
  goal: minimize
  name: value_loss

parameters:
  # Environment parameters
  env_name:
    value: VMAS
  scenario_name:
    value: navigation
  num_agents:
    value: 2
  episode_length:
    value: 100
  
  # Algorithm parameters
  algorithm_name:
    value: rmappo
  experiment_name:
    value: sweep_rmappo
  seed:
    value: 1
  
  # Training parameters
  n_training_threads:
    value: 1
  n_rollout_threads:
    value: 8
  num_env_steps:
    value: 10000000
  
  # Network parameters (similar to hidden_dim in DISSCv2)
  hidden_size:
    values: [64, 128]
  
  # Learning rate (similar to DISSCv2)
  lr:
    values: [0.0001, 0.0003, 0.0005]
  critic_lr:
    values: [0.0001, 0.0003, 0.0005]
  
  # Reward normalization (similar to standardise_rewards in DISSCv2)
  use_valuenorm:
    values: [true, false]
  use_popart:
    values: [true, false]
  
  # Recurrent policy (similar to use_rnn in DISSCv2)
  use_recurrent_policy:
    values: [true, false]
  
  # Entropy coefficient (same as DISSCv2)
  entropy_coef:
    values: [0.01, 0.001]
  
  # DAE specific parameters
  beta:
    values: [0.0, 0.1, 0.5, 1.0]
  rew_hidden_size:
    values: [64, 128, 256]
  num_rew:
    values: [1, 2, 3]
  
  # Fixed parameters
  use_ReLU:
    value: true
  use_feature_normalization:
    value: true
  use_orthogonal:
    value: true
  use_centralized_V:
    value: true
  share_policy:
    value: true
  use_wandb:
    value: true
  user_name:
    value: mappo
  cuda:
    value: true
  cuda_deterministic:
    value: true

